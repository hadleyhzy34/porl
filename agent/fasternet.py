# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
import torch
import torch.nn as nn
import math
# from timm.models.layers import DropPath, to_2tuple, trunc_normal_
from functools import partial
from typing import List
from torch import Tensor
import copy
import os
import pdb
from util.costmap import state2costmap


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob > 0.0 and scale_by_keep:
        random_tensor.div_(keep_prob)
    return x * random_tensor


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(agent, drop_prob: float = 0., scale_by_keep: bool = True):
        super(DropPath, agent).__init__()
        agent.drop_prob = drop_prob
        agent.scale_by_keep = scale_by_keep

    def forward(agent, x):
        return drop_path(x, agent.drop_prob, agent.training, agent.scale_by_keep)

    def extra_repr(agent):
        return f'drop_prob={round(agent.drop_prob,3):0.3f}'

class Partial_conv3(nn.Module):

    def __init__(agent, dim, n_div, forward):
        super().__init__()
        agent.dim_conv3 = dim // n_div
        agent.dim_untouched = dim - agent.dim_conv3
        agent.partial_conv3 = nn.Conv2d(agent.dim_conv3, agent.dim_conv3, 3, 1, 1, bias=False)

        if forward == 'slicing':
            agent.forward = agent.forward_slicing
        elif forward == 'split_cat':
            agent.forward = agent.forward_split_cat
        else:
            raise NotImplementedError

    def forward_slicing(agent, x: Tensor) -> Tensor:
        # only for inference
        x = x.clone()   # !!! Keep the original input intact for the residual connection later
        x[:, :agent.dim_conv3, :, :] = agent.partial_conv3(x[:, :agent.dim_conv3, :, :])

        return x

    def forward_split_cat(agent, x: Tensor) -> Tensor:
        # for training/inference
        x1, x2 = torch.split(x, [agent.dim_conv3, agent.dim_untouched], dim=1)
        x1 = agent.partial_conv3(x1)
        x = torch.cat((x1, x2), 1)

        return x


class MLPBlock(nn.Module):

    def __init__(agent,
                 dim,
                 n_div,
                 mlp_ratio,
                 drop_path,
                 layer_scale_init_value,
                 act_layer,
                 norm_layer,
                 pconv_fw_type
                 ):

        super().__init__()
        agent.dim = dim
        agent.mlp_ratio = mlp_ratio
        agent.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        agent.n_div = n_div

        mlp_hidden_dim = int(dim * mlp_ratio)

        mlp_layer: List[nn.Module] = [
            nn.Conv2d(dim, mlp_hidden_dim, 1, bias=False),
            norm_layer(mlp_hidden_dim),
            act_layer(),
            nn.Conv2d(mlp_hidden_dim, dim, 1, bias=False)
        ]

        agent.mlp = nn.Sequential(*mlp_layer)

        agent.spatial_mixing = Partial_conv3(
            dim,
            n_div,
            pconv_fw_type
        )

        if layer_scale_init_value > 0:
            agent.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)
            agent.forward = agent.forward_layer_scale
        else:
            agent.forward = agent.forward

    def forward(agent, x: Tensor) -> Tensor:
        shortcut = x
        x = agent.spatial_mixing(x)
        x = shortcut + agent.drop_path(agent.mlp(x))
        return x

    def forward_layer_scale(agent, x: Tensor) -> Tensor:
        shortcut = x
        x = agent.spatial_mixing(x)
        x = shortcut + agent.drop_path(
            agent.layer_scale.unsqueeze(-1).unsqueeze(-1) * agent.mlp(x))
        return x


class BasicStage(nn.Module):

    def __init__(agent,
                 dim,
                 depth,
                 n_div,
                 mlp_ratio,
                 drop_path,
                 layer_scale_init_value,
                 norm_layer,
                 act_layer,
                 pconv_fw_type
                 ):

        super().__init__()

        blocks_list = [
            MLPBlock(
                dim=dim,
                n_div=n_div,
                mlp_ratio=mlp_ratio,
                drop_path=drop_path[i],
                layer_scale_init_value=layer_scale_init_value,
                norm_layer=norm_layer,
                act_layer=act_layer,
                pconv_fw_type=pconv_fw_type
            )
            for i in range(depth)
        ]

        agent.blocks = nn.Sequential(*blocks_list)

    def forward(agent, x: Tensor) -> Tensor:
        x = agent.blocks(x)
        return x


class PatchEmbed(nn.Module):

    def __init__(agent, patch_size, patch_stride, in_chans, embed_dim, norm_layer):
        super().__init__()
        agent.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_stride, bias=False)
        if norm_layer is not None:
            agent.norm = norm_layer(embed_dim)
        else:
            agent.norm = nn.Identity()

    def forward(agent, x: Tensor) -> Tensor:
        x = agent.norm(agent.proj(x))
        return x


class PatchMerging(nn.Module):

    def __init__(agent, patch_size2, patch_stride2, dim, norm_layer):
        super().__init__()
        agent.reduction = nn.Conv2d(dim, 2 * dim, kernel_size=patch_size2, stride=patch_stride2, bias=False)
        if norm_layer is not None:
            agent.norm = norm_layer(2 * dim)
        else:
            agent.norm = nn.Identity()

    def forward(agent, x: Tensor) -> Tensor:
        x = agent.norm(agent.reduction(x))
        return x


class FasterNet(nn.Module):

    def __init__(agent,
                 in_chans=3,
                 num_classes=1000,
                 embed_dim=96,
                 # depths=(1, 2, 8, 2),
                 depths=(1,2),
                 mlp_ratio=2.,
                 n_div=4,
                 patch_size=4,
                 patch_stride=4,
                 patch_size2=2,  # for subsequent layers
                 patch_stride2=2,
                 patch_norm=True,
                 feature_dim=1280,
                 drop_path_rate=0.1,
                 layer_scale_init_value=0,
                 norm_layer='BN',
                 act_layer='RELU',
                 fork_feat=False,
                 init_cfg=None,
                 pretrained=None,
                 pconv_fw_type='split_cat',
                 **kwargs):
        super().__init__()

        if norm_layer == 'BN':
            norm_layer = nn.BatchNorm2d
        else:
            raise NotImplementedError

        if act_layer == 'GELU':
            act_layer = nn.GELU
        elif act_layer == 'RELU':
            act_layer = partial(nn.ReLU, inplace=True)
        else:
            raise NotImplementedError

        if not fork_feat:
            agent.num_classes = num_classes
        agent.num_stages = len(depths)
        agent.embed_dim = embed_dim
        agent.patch_norm = patch_norm
        agent.num_features = int(embed_dim * 2 ** (agent.num_stages - 1))
        agent.mlp_ratio = mlp_ratio
        agent.depths = depths

        # split image into non-overlapping patches
        agent.patch_embed = PatchEmbed(
            patch_size=patch_size,
            patch_stride=patch_stride,
            in_chans=in_chans,
            embed_dim=embed_dim,
            norm_layer=norm_layer if agent.patch_norm else None
        )

        # stochastic depth decay rule
        dpr = [x.item()
               for x in torch.linspace(0, drop_path_rate, sum(depths))]

        # build layers
        stages_list = []
        for i_stage in range(agent.num_stages):
            stage = BasicStage(dim=int(embed_dim * 2 ** i_stage),
                               n_div=n_div,
                               depth=depths[i_stage],
                               mlp_ratio=agent.mlp_ratio,
                               drop_path=dpr[sum(depths[:i_stage]):sum(depths[:i_stage + 1])],
                               layer_scale_init_value=layer_scale_init_value,
                               norm_layer=norm_layer,
                               act_layer=act_layer,
                               pconv_fw_type=pconv_fw_type
                               )
            stages_list.append(stage)

            # patch merging layer
            if i_stage < agent.num_stages - 1:
                stages_list.append(
                    PatchMerging(patch_size2=patch_size2,
                                 patch_stride2=patch_stride2,
                                 dim=int(embed_dim * 2 ** i_stage),
                                 norm_layer=norm_layer)
                )

        agent.stages = nn.Sequential(*stages_list)

        agent.fork_feat = fork_feat

        if agent.fork_feat:
            agent.forward = agent.forward_det
            # add a norm layer for each output
            agent.out_indices = [0, 2, 4, 6]
            for i_emb, i_layer in enumerate(agent.out_indices):
                if i_emb == 0 and os.environ.get('FORK_LAST3', None):
                    raise NotImplementedError
                else:
                    layer = norm_layer(int(embed_dim * 2 ** i_emb))
                layer_name = f'norm{i_layer}'
                agent.add_module(layer_name, layer)
        else:
            agent.forward = agent.forward_cls
            # Classifier head
            agent.avgpool_pre_head = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(agent.num_features, feature_dim, 1, bias=False),
                act_layer()
            )
            agent.head = nn.Linear(feature_dim, num_classes) \
                if num_classes > 0 else nn.Identity()

        agent.apply(agent.cls_init_weights)
        agent.init_cfg = copy.deepcopy(init_cfg)
        if agent.fork_feat and (agent.init_cfg is not None or pretrained is not None):
            agent.init_weights()

    def cls_init_weights(agent, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.Conv1d, nn.Conv2d)):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.GroupNorm)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    # init for mmdetection by loading imagenet pre-trained weights
    def init_weights(agent, pretrained=None):
        logger = get_root_logger()
        if agent.init_cfg is None and pretrained is None:
            logger.warn(f'No pre-trained weights for '
                        f'{agent.__class__.__name__}, '
                        f'training start from scratch')
            pass
        else:
            assert 'checkpoint' in agent.init_cfg, f'Only support ' \
                                                  f'specify `Pretrained` in ' \
                                                  f'`init_cfg` in ' \
                                                  f'{agent.__class__.__name__} '
            if agent.init_cfg is not None:
                ckpt_path = agent.init_cfg['checkpoint']
            elif pretrained is not None:
                ckpt_path = pretrained

            ckpt = _load_checkpoint(
                ckpt_path, logger=logger, map_location='cpu')
            if 'state_dict' in ckpt:
                _state_dict = ckpt['state_dict']
            elif 'model' in ckpt:
                _state_dict = ckpt['model']
            else:
                _state_dict = ckpt

            state_dict = _state_dict
            missing_keys, unexpected_keys = \
                agent.load_state_dict(state_dict, False)

            # show for debug
            print('missing_keys: ', missing_keys)
            print('unexpected_keys: ', unexpected_keys)

    def forward_cls(agent, x):
        # output only the features of last layer for image classification
        # pdb.set_trace()
        x = state2costmap(x)
        x = agent.patch_embed(x)
        x = agent.stages(x)
        x = agent.avgpool_pre_head(x)  # B C 1 1
        x = torch.flatten(x, 1)
        x = agent.head(x)

        return x

    def forward_det(agent, x: Tensor) -> Tensor:
        # output the features of four stages for dense prediction
        x = agent.patch_embed(x)
        outs = []
        for idx, stage in enumerate(agent.stages):
            x = stage(x)
            if agent.fork_feat and idx in agent.out_indices:
                norm_layer = getattr(agent, f'norm{idx}')
                x_out = norm_layer(x)
                outs.append(x_out)

        return outs

if __name__ == "__main__":
    model = FasterNet(3,256)
    input = torch.rand(5,3,360,256)
    pdb.set_trace()
    output = model.forward_cls(input)
    print(model)
